{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBGkiWikeNuD",
        "outputId": "0db36344-b52e-4de9-9869-8c41fb99e5a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX7ZINCymr2D",
        "outputId": "39e59646-bc13-4bba-d1f7-d497bd44ce91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9ZSKkJQldAQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import transformers\n",
        "from transformers import RobertaModel, RobertaTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Settings:\n",
        "    batch_size=100\n",
        "    max_len=350\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    seed = 768"
      ],
      "metadata": {
        "id": "CReinpPpluxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainValidDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.df = df\n",
        "        self.text = df[\"tweet\"].values\n",
        "        self.target = df[\"label\"].values\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        texts = self.text[idx]\n",
        "        tokenized = self.tokenizer.encode_plus(texts, truncation=True, add_special_tokens=True,\n",
        "                                               max_length=self.max_len, padding=\"max_length\")\n",
        "        ids = tokenized[\"input_ids\"]\n",
        "        mask = tokenized[\"attention_mask\"]\n",
        "        targets = self.target[idx]\n",
        "        return {\n",
        "            \"ids\": torch.LongTensor(ids),\n",
        "            \"mask\": torch.LongTensor(mask),\n",
        "            \"targets\": torch.tensor(targets, dtype=torch.float32)\n",
        "        }"
      ],
      "metadata": {
        "id": "fG5BloVum0sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CommonLitRoBERTa(nn.Module):\n",
        "    def __init__(self, pretrained_path):\n",
        "        super().__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(pretrained_path)\n",
        "        \n",
        "    def forward(self, ids, mask):\n",
        "        output = self.roberta(ids, attention_mask=mask)\n",
        "        return output"
      ],
      "metadata": {
        "id": "wm2g7mMQm5sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CommonLitRoBERTa(\"roberta-base\")\n",
        "model.to(Settings.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zhub872_m7rP",
        "outputId": "61d1a298-ac7d-41fd-8b7e-dbb1d7fb6b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommonLitRoBERTa(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaMeqKi6m_Mt",
        "outputId": "6fa83d4b-53c8-47b3-cfde-9a858670658b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreTrainedTokenizer(name_or_path='roberta-base', vocab_size=50265, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datatweet = pd.read_csv(\"DataSet.csv\")\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "datatweet.label = le.fit_transform(datatweet.label.values)"
      ],
      "metadata": {
        "id": "4X4gWQ610C2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(datatweet, test_size=0.2, random_state=42)\n",
        "\n",
        "train.to_csv(\"train_df.csv\")\n",
        "\n",
        "test.to_csv(\"test_df.csv\")"
      ],
      "metadata": {
        "id": "OzsU2T5rmTGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datatweet.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgmanwCvXQNc",
        "outputId": "5544d781-84d4-46b1-8721-8e6a9d70dcd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(49, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datatweet.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "hnMh-66gWNQO",
        "outputId": "807706b8-de3a-4f24-9af7-e050248c48f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id_akun  label      username  \\\n",
              "0        1      0        12290F   \n",
              "1        2      0       abiyoga   \n",
              "2        3      0  adityarestya   \n",
              "3        4      0   akhmad arie   \n",
              "4        5      0  aldryandimas   \n",
              "\n",
              "                                               tweet  \n",
              "0  its besar salah for me butuh kj to mtk anjing ...  \n",
              "1  if you like pineapple and you like pizza you s...  \n",
              "2  mbak analogi nya ting logika pikir nya lebih t...  \n",
              "3  iri keren daftar kuis nama ariyanto alamat kot...  \n",
              "4  standar ganda salah salah ayo ajar me iya sepe...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f3938e5f-7e1e-46dc-821b-fdb652cc32c5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_akun</th>\n",
              "      <th>label</th>\n",
              "      <th>username</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>12290F</td>\n",
              "      <td>its besar salah for me butuh kj to mtk anjing ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>abiyoga</td>\n",
              "      <td>if you like pineapple and you like pizza you s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>adityarestya</td>\n",
              "      <td>mbak analogi nya ting logika pikir nya lebih t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>akhmad arie</td>\n",
              "      <td>iri keren daftar kuis nama ariyanto alamat kot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>aldryandimas</td>\n",
              "      <td>standar ganda salah salah ayo ajar me iya sepe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3938e5f-7e1e-46dc-821b-fdb652cc32c5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f3938e5f-7e1e-46dc-821b-fdb652cc32c5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f3938e5f-7e1e-46dc-821b-fdb652cc32c5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"train_df.csv\")\n",
        "df_train = df_train.dropna()\n",
        "df_test = pd.read_csv(\"test_df.csv\")\n",
        "df_test = df_test.dropna()"
      ],
      "metadata": {
        "id": "QcfL9SC2mXWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the datasets\n",
        "\n",
        "# df_test = pd.read_csv(\"test_df.csv\")"
      ],
      "metadata": {
        "id": "67juwr_L0XUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train = pd.read_csv(\"train_df.csv\")\n",
        "\n",
        "train_dataset = TrainValidDataset(df_train, tokenizer, Settings.max_len)\n",
        "train_loader = DataLoader(train_dataset, batch_size=Settings.batch_size,\n",
        "                          shuffle=True, num_workers=8, pin_memory=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFj0neuo7qf7",
        "outputId": "8ec16442-13ae-481d-f803-6942410aad2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make mini batch data\n",
        "\n",
        "batch = next(iter(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s-EdabR_a3b",
        "outputId": "8c53f03c-b44b-4c95-ace7-9c5cac7d4686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABCpUHoVYx7W",
        "outputId": "15450f78-97e5-4ed8-94af-44c660323d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': tensor([[    0, 19807,  6713,  ...,  1115,    50,     2],\n",
              "         [    0, 48883, 36091,  ..., 47510, 44919,     2],\n",
              "         [    0,  1694, 10905,  ...,  1908,   417,     2],\n",
              "         ...,\n",
              "         [    0,   428,  1115,  ...,     7,  2231,     2],\n",
              "         [    0, 40577, 40577,  ..., 15116,  5186,     2],\n",
              "         [    0,    90,   808,  ..., 11877,   295,     2]]),\n",
              " 'mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
              " 'targets': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = batch[\"ids\"].to(Settings.device)\n",
        "mask = batch[\"mask\"].to(Settings.device)\n",
        "targets = batch[\"targets\"].to(Settings.device)\n",
        "\n",
        "print(ids.shape)\n",
        "print(mask.shape)\n",
        "print(targets.shape)"
      ],
      "metadata": {
        "id": "y-nEPvoKnFRy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40b6f997-2031-4a88-cab0-26933fa616cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([39, 350])\n",
            "torch.Size([39, 350])\n",
            "torch.Size([39])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(ids, mask)\n",
        "output"
      ],
      "metadata": {
        "id": "GjwrvGo-nGXj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "610c3e0a-fa5d-43c2-b6b5-024c142c3630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions([('last_hidden_state',\n",
              "                                               tensor([[[-0.0497,  0.0501,  0.0395,  ..., -0.1671, -0.0434,  0.0124],\n",
              "                                                        [ 0.0870,  0.0581, -0.0286,  ..., -0.2379,  0.1798, -0.0914],\n",
              "                                                        [ 0.0260,  0.0976,  0.0097,  ...,  0.1993, -0.0131,  0.1843],\n",
              "                                                        ...,\n",
              "                                                        [ 0.0827,  0.0518,  0.1175,  ..., -0.2603,  0.0658,  0.0021],\n",
              "                                                        [ 0.1608, -0.2326,  0.1314,  ...,  0.2852, -0.0720,  0.0396],\n",
              "                                                        [-0.0449,  0.0426,  0.0308,  ..., -0.1954, -0.0366,  0.0015]],\n",
              "                                               \n",
              "                                                       [[-0.0485,  0.0495,  0.0350,  ..., -0.1780,  0.0022,  0.0390],\n",
              "                                                        [ 0.0083,  0.1814,  0.0594,  ..., -0.2054,  0.1259,  0.0629],\n",
              "                                                        [-0.1482,  0.0376, -0.0357,  ..., -0.0130, -0.1401,  0.0959],\n",
              "                                                        ...,\n",
              "                                                        [ 0.0257,  0.0037,  0.0405,  ..., -0.4509,  0.0528,  0.1105],\n",
              "                                                        [-0.0110,  0.0503, -0.0720,  ..., -0.3228,  0.0430,  0.1063],\n",
              "                                                        [-0.0436,  0.0448,  0.0261,  ..., -0.2037,  0.0078,  0.0226]],\n",
              "                                               \n",
              "                                                       [[-0.0329,  0.0188,  0.0410,  ..., -0.1831, -0.0335,  0.0314],\n",
              "                                                        [ 0.1192, -0.0326,  0.0600,  ..., -0.0639,  0.0588,  0.2271],\n",
              "                                                        [ 0.0648,  0.0373,  0.1766,  ..., -0.1894,  0.0444,  0.2830],\n",
              "                                                        ...,\n",
              "                                                        [-0.0251, -0.0976,  0.1574,  ..., -0.9558, -0.0516,  0.0663],\n",
              "                                                        [-0.1089, -0.1636,  0.2234,  ..., -0.1092,  0.0334, -0.0180],\n",
              "                                                        [-0.0285,  0.0152,  0.0317,  ..., -0.2055, -0.0280,  0.0136]],\n",
              "                                               \n",
              "                                                       ...,\n",
              "                                               \n",
              "                                                       [[-0.0550,  0.0254,  0.0378,  ..., -0.1903, -0.0359,  0.0226],\n",
              "                                                        [-0.1758,  0.1044,  0.0545,  ...,  0.0557,  0.1511,  0.0402],\n",
              "                                                        [-0.1972,  0.1117,  0.0382,  ..., -0.3503,  0.1575, -0.1529],\n",
              "                                                        ...,\n",
              "                                                        [-0.1146, -0.1236,  0.0775,  ..., -0.3066, -0.1596, -0.0213],\n",
              "                                                        [ 0.0112, -0.0315, -0.0036,  ..., -0.2642,  0.0467, -0.1339],\n",
              "                                                        [-0.0488,  0.0233,  0.0332,  ..., -0.2138, -0.0295,  0.0118]],\n",
              "                                               \n",
              "                                                       [[-0.0437,  0.0516,  0.0565,  ..., -0.1419, -0.0319,  0.0361],\n",
              "                                                        [ 0.0405,  0.0128, -0.0849,  ..., -0.5961,  0.0521,  0.0690],\n",
              "                                                        [ 0.1470,  0.0410,  0.0705,  ..., -0.3193,  0.0501,  0.0702],\n",
              "                                                        ...,\n",
              "                                                        [-0.0132,  0.1152,  0.1529,  ..., -0.0867, -0.1477,  0.0085],\n",
              "                                                        [ 0.0644,  0.2830,  0.1113,  ..., -0.7151,  0.0344, -0.0481],\n",
              "                                                        [-0.0418,  0.0486,  0.0510,  ..., -0.1619, -0.0259,  0.0187]],\n",
              "                                               \n",
              "                                                       [[-0.0575,  0.0030,  0.0341,  ..., -0.2018, -0.0151, -0.0086],\n",
              "                                                        [-0.1636, -0.1779,  0.0188,  ..., -0.2593,  0.2245,  0.1498],\n",
              "                                                        [-0.1240,  0.1406,  0.0936,  ..., -0.2302,  0.2248,  0.1403],\n",
              "                                                        ...,\n",
              "                                                        [ 0.0835,  0.0772,  0.1432,  ..., -0.6077,  0.0143,  0.0117],\n",
              "                                                        [ 0.0644, -0.0599,  0.1502,  ..., -0.2453, -0.0287,  0.0535],\n",
              "                                                        [-0.0493, -0.0029,  0.0270,  ..., -0.2274, -0.0098, -0.0239]]],\n",
              "                                                      grad_fn=<NativeLayerNormBackward0>)),\n",
              "                                              ('pooler_output',\n",
              "                                               tensor([[-0.0574, -0.2461, -0.1871,  ..., -0.1188, -0.0568, -0.1148],\n",
              "                                                       [-0.0604, -0.2628, -0.1877,  ..., -0.1101, -0.0555, -0.1189],\n",
              "                                                       [-0.0411, -0.2225, -0.1949,  ..., -0.1166, -0.0406, -0.1088],\n",
              "                                                       ...,\n",
              "                                                       [-0.0492, -0.2684, -0.1915,  ..., -0.1148, -0.0617, -0.1153],\n",
              "                                                       [-0.0471, -0.2512, -0.2088,  ..., -0.1133, -0.0685, -0.0952],\n",
              "                                                       [-0.0417, -0.2584, -0.1810,  ..., -0.1200, -0.0531, -0.1013]],\n",
              "                                                      grad_fn=<TanhBackward0>))])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# last_hidden_state\n",
        "last_hidden_state = output[0]\n",
        "print(\"shape:\", last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "vxhrSmSknH3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e351c568-ed7d-4614-b4f7-9c0cd4242dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: torch.Size([39, 350, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pooler output\n",
        "pooler_output = output[1]\n",
        "print(\"shape:\", pooler_output.shape)"
      ],
      "metadata": {
        "id": "elsQ15r6nJMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28bd6284-9f7b-452d-9744-7cf8aa0eacdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: torch.Size([39, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cls_embeddings = last_hidden_state[:, 0, :].detach()\n",
        "\n",
        "print(\"shape:\", cls_embeddings.shape)\n",
        "print(\"\")\n",
        "print(cls_embeddings)"
      ],
      "metadata": {
        "id": "Fk2An8MSnN_y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd39212-54a6-4605-8c91-a0a11e161a59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: torch.Size([39, 768])\n",
            "\n",
            "tensor([[-0.0497,  0.0501,  0.0395,  ..., -0.1671, -0.0434,  0.0124],\n",
            "        [-0.0485,  0.0495,  0.0350,  ..., -0.1780,  0.0022,  0.0390],\n",
            "        [-0.0329,  0.0188,  0.0410,  ..., -0.1831, -0.0335,  0.0314],\n",
            "        ...,\n",
            "        [-0.0550,  0.0254,  0.0378,  ..., -0.1903, -0.0359,  0.0226],\n",
            "        [-0.0437,  0.0516,  0.0565,  ..., -0.1419, -0.0319,  0.0361],\n",
            "        [-0.0575,  0.0030,  0.0341,  ..., -0.2018, -0.0151, -0.0086]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(cls_embeddings.cpu().numpy()).head()"
      ],
      "metadata": {
        "id": "m5FzPelBnPCt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "ccaa6606-b469-4df2-8451-75271618741c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0         1         2         3         4         5         6    \\\n",
              "0 -0.049687  0.050111  0.039524 -0.117616  0.009222 -0.191291  0.015327   \n",
              "1 -0.048485  0.049538  0.034984 -0.092724 -0.040460 -0.223916  0.015657   \n",
              "2 -0.032903  0.018809  0.041032 -0.097800  0.013628 -0.187409  0.006830   \n",
              "3 -0.031474  0.018772  0.032856 -0.106333  0.015122 -0.182310 -0.005653   \n",
              "4 -0.052631  0.030343  0.039420 -0.082525 -0.032423 -0.194593  0.006505   \n",
              "\n",
              "        7         8         9    ...       758       759       760       761  \\\n",
              "0  0.016254  0.013182 -0.019113  ...  0.100256  0.028469 -0.131216 -0.079339   \n",
              "1  0.039550  0.010930 -0.044281  ...  0.104432  0.041962 -0.114383 -0.088692   \n",
              "2  0.082632  0.023073 -0.072869  ...  0.094917  0.050176 -0.161191 -0.073268   \n",
              "3  0.050389  0.034509 -0.052329  ...  0.105500  0.055012 -0.154690 -0.107367   \n",
              "4  0.025780  0.027278 -0.042045  ...  0.101105  0.042587 -0.091588 -0.060145   \n",
              "\n",
              "        762       763       764       765       766       767  \n",
              "0 -0.059993 -0.031143  0.159584 -0.167118 -0.043415  0.012353  \n",
              "1 -0.047583 -0.014563  0.167224 -0.178036  0.002216  0.039033  \n",
              "2 -0.104327  0.006934  0.137702 -0.183100 -0.033492  0.031351  \n",
              "3 -0.056426  0.015322  0.178858 -0.215475 -0.033499  0.010702  \n",
              "4 -0.059455 -0.007481  0.155557 -0.211936 -0.053768 -0.016025  \n",
              "\n",
              "[5 rows x 768 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f9cd6b2-a7db-45b7-8dd6-6de56fd2100d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.049687</td>\n",
              "      <td>0.050111</td>\n",
              "      <td>0.039524</td>\n",
              "      <td>-0.117616</td>\n",
              "      <td>0.009222</td>\n",
              "      <td>-0.191291</td>\n",
              "      <td>0.015327</td>\n",
              "      <td>0.016254</td>\n",
              "      <td>0.013182</td>\n",
              "      <td>-0.019113</td>\n",
              "      <td>...</td>\n",
              "      <td>0.100256</td>\n",
              "      <td>0.028469</td>\n",
              "      <td>-0.131216</td>\n",
              "      <td>-0.079339</td>\n",
              "      <td>-0.059993</td>\n",
              "      <td>-0.031143</td>\n",
              "      <td>0.159584</td>\n",
              "      <td>-0.167118</td>\n",
              "      <td>-0.043415</td>\n",
              "      <td>0.012353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.048485</td>\n",
              "      <td>0.049538</td>\n",
              "      <td>0.034984</td>\n",
              "      <td>-0.092724</td>\n",
              "      <td>-0.040460</td>\n",
              "      <td>-0.223916</td>\n",
              "      <td>0.015657</td>\n",
              "      <td>0.039550</td>\n",
              "      <td>0.010930</td>\n",
              "      <td>-0.044281</td>\n",
              "      <td>...</td>\n",
              "      <td>0.104432</td>\n",
              "      <td>0.041962</td>\n",
              "      <td>-0.114383</td>\n",
              "      <td>-0.088692</td>\n",
              "      <td>-0.047583</td>\n",
              "      <td>-0.014563</td>\n",
              "      <td>0.167224</td>\n",
              "      <td>-0.178036</td>\n",
              "      <td>0.002216</td>\n",
              "      <td>0.039033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.032903</td>\n",
              "      <td>0.018809</td>\n",
              "      <td>0.041032</td>\n",
              "      <td>-0.097800</td>\n",
              "      <td>0.013628</td>\n",
              "      <td>-0.187409</td>\n",
              "      <td>0.006830</td>\n",
              "      <td>0.082632</td>\n",
              "      <td>0.023073</td>\n",
              "      <td>-0.072869</td>\n",
              "      <td>...</td>\n",
              "      <td>0.094917</td>\n",
              "      <td>0.050176</td>\n",
              "      <td>-0.161191</td>\n",
              "      <td>-0.073268</td>\n",
              "      <td>-0.104327</td>\n",
              "      <td>0.006934</td>\n",
              "      <td>0.137702</td>\n",
              "      <td>-0.183100</td>\n",
              "      <td>-0.033492</td>\n",
              "      <td>0.031351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.031474</td>\n",
              "      <td>0.018772</td>\n",
              "      <td>0.032856</td>\n",
              "      <td>-0.106333</td>\n",
              "      <td>0.015122</td>\n",
              "      <td>-0.182310</td>\n",
              "      <td>-0.005653</td>\n",
              "      <td>0.050389</td>\n",
              "      <td>0.034509</td>\n",
              "      <td>-0.052329</td>\n",
              "      <td>...</td>\n",
              "      <td>0.105500</td>\n",
              "      <td>0.055012</td>\n",
              "      <td>-0.154690</td>\n",
              "      <td>-0.107367</td>\n",
              "      <td>-0.056426</td>\n",
              "      <td>0.015322</td>\n",
              "      <td>0.178858</td>\n",
              "      <td>-0.215475</td>\n",
              "      <td>-0.033499</td>\n",
              "      <td>0.010702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.052631</td>\n",
              "      <td>0.030343</td>\n",
              "      <td>0.039420</td>\n",
              "      <td>-0.082525</td>\n",
              "      <td>-0.032423</td>\n",
              "      <td>-0.194593</td>\n",
              "      <td>0.006505</td>\n",
              "      <td>0.025780</td>\n",
              "      <td>0.027278</td>\n",
              "      <td>-0.042045</td>\n",
              "      <td>...</td>\n",
              "      <td>0.101105</td>\n",
              "      <td>0.042587</td>\n",
              "      <td>-0.091588</td>\n",
              "      <td>-0.060145</td>\n",
              "      <td>-0.059455</td>\n",
              "      <td>-0.007481</td>\n",
              "      <td>0.155557</td>\n",
              "      <td>-0.211936</td>\n",
              "      <td>-0.053768</td>\n",
              "      <td>-0.016025</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 768 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f9cd6b2-a7db-45b7-8dd6-6de56fd2100d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0f9cd6b2-a7db-45b7-8dd6-6de56fd2100d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0f9cd6b2-a7db-45b7-8dd6-6de56fd2100d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(cls_embeddings.cpu().numpy()).shape"
      ],
      "metadata": {
        "id": "mEeQsdaQbfgI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c259eab9-e8d5-4720-87bc-728a93ac4feb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_hidden_state.shape"
      ],
      "metadata": {
        "id": "i9BtlbbrnQC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56df3ff-c0da-4477-f0ac-be11bd1e9647"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([39, 350, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# apply avg.pooling to word embeddings\n",
        "# 単語埋め込みベクトルにaverage pooling を適用する\n",
        "pooled_embeddings = last_hidden_state.detach().mean(dim=1)\n",
        "\n",
        "print(\"shape:\", pooled_embeddings.shape)\n",
        "print(\"\")\n",
        "print(pooled_embeddings)"
      ],
      "metadata": {
        "id": "l0MrrShonQye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ac69d55-dc0e-40eb-fe5a-0f8464245a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: torch.Size([39, 768])\n",
            "\n",
            "tensor([[-0.0865,  0.0199,  0.0868,  ..., -0.2682, -0.0441, -0.0028],\n",
            "        [-0.0539,  0.0589,  0.0742,  ..., -0.2769,  0.0161,  0.0616],\n",
            "        [ 0.0096, -0.0109,  0.1060,  ..., -0.3105, -0.0092,  0.0276],\n",
            "        ...,\n",
            "        [-0.0581,  0.0028,  0.1000,  ..., -0.3127, -0.0024,  0.0096],\n",
            "        [-0.0078,  0.0389,  0.1051,  ..., -0.1949, -0.0029,  0.0579],\n",
            "        [-0.0689,  0.0043,  0.0958,  ..., -0.3116,  0.0035,  0.0054]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(pooled_embeddings.cpu().numpy()).head()"
      ],
      "metadata": {
        "id": "yz8LEUsknR1F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "4ab5c8c3-5517-4bdb-8a6e-8498f3c2faf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0         1         2         3         4         5         6    \\\n",
              "0 -0.086476  0.019859  0.086814  0.118813 -0.086344 -0.291477 -0.056506   \n",
              "1 -0.053920  0.058911  0.074153  0.092516 -0.224913 -0.280537 -0.008037   \n",
              "2  0.009632 -0.010867  0.106044  0.101713 -0.078762 -0.205160 -0.001309   \n",
              "3 -0.075851 -0.028208  0.083660  0.123008 -0.100873 -0.229472 -0.039923   \n",
              "4 -0.068081  0.044345  0.106599  0.117576 -0.236935 -0.240149 -0.052856   \n",
              "\n",
              "        7         8         9    ...       758       759       760       761  \\\n",
              "0  0.047834 -0.001696 -0.017935  ...  0.182298 -0.032340 -0.194552  0.054748   \n",
              "1  0.036741 -0.009352 -0.043444  ...  0.114547  0.013849 -0.131253  0.027463   \n",
              "2  0.106982  0.034212 -0.044682  ...  0.094439  0.016506 -0.151334  0.037902   \n",
              "3  0.030180 -0.010829 -0.030644  ...  0.143720  0.034368 -0.238438  0.020200   \n",
              "4  0.050336 -0.007994 -0.036271  ...  0.153078 -0.010070 -0.166419  0.039150   \n",
              "\n",
              "        762       763       764       765       766       767  \n",
              "0 -0.062074 -0.205619  0.318559 -0.268161 -0.044116 -0.002827  \n",
              "1 -0.069676 -0.157081  0.359347 -0.276895  0.016094  0.061609  \n",
              "2 -0.120236 -0.121229  0.344951 -0.310451 -0.009200  0.027640  \n",
              "3 -0.074293 -0.142198  0.386423 -0.360985 -0.013981 -0.013176  \n",
              "4 -0.072276 -0.174403  0.278976 -0.307429 -0.040699 -0.012276  \n",
              "\n",
              "[5 rows x 768 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec516557-7867-44bd-97c2-32f3efeec93f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.086476</td>\n",
              "      <td>0.019859</td>\n",
              "      <td>0.086814</td>\n",
              "      <td>0.118813</td>\n",
              "      <td>-0.086344</td>\n",
              "      <td>-0.291477</td>\n",
              "      <td>-0.056506</td>\n",
              "      <td>0.047834</td>\n",
              "      <td>-0.001696</td>\n",
              "      <td>-0.017935</td>\n",
              "      <td>...</td>\n",
              "      <td>0.182298</td>\n",
              "      <td>-0.032340</td>\n",
              "      <td>-0.194552</td>\n",
              "      <td>0.054748</td>\n",
              "      <td>-0.062074</td>\n",
              "      <td>-0.205619</td>\n",
              "      <td>0.318559</td>\n",
              "      <td>-0.268161</td>\n",
              "      <td>-0.044116</td>\n",
              "      <td>-0.002827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.053920</td>\n",
              "      <td>0.058911</td>\n",
              "      <td>0.074153</td>\n",
              "      <td>0.092516</td>\n",
              "      <td>-0.224913</td>\n",
              "      <td>-0.280537</td>\n",
              "      <td>-0.008037</td>\n",
              "      <td>0.036741</td>\n",
              "      <td>-0.009352</td>\n",
              "      <td>-0.043444</td>\n",
              "      <td>...</td>\n",
              "      <td>0.114547</td>\n",
              "      <td>0.013849</td>\n",
              "      <td>-0.131253</td>\n",
              "      <td>0.027463</td>\n",
              "      <td>-0.069676</td>\n",
              "      <td>-0.157081</td>\n",
              "      <td>0.359347</td>\n",
              "      <td>-0.276895</td>\n",
              "      <td>0.016094</td>\n",
              "      <td>0.061609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.009632</td>\n",
              "      <td>-0.010867</td>\n",
              "      <td>0.106044</td>\n",
              "      <td>0.101713</td>\n",
              "      <td>-0.078762</td>\n",
              "      <td>-0.205160</td>\n",
              "      <td>-0.001309</td>\n",
              "      <td>0.106982</td>\n",
              "      <td>0.034212</td>\n",
              "      <td>-0.044682</td>\n",
              "      <td>...</td>\n",
              "      <td>0.094439</td>\n",
              "      <td>0.016506</td>\n",
              "      <td>-0.151334</td>\n",
              "      <td>0.037902</td>\n",
              "      <td>-0.120236</td>\n",
              "      <td>-0.121229</td>\n",
              "      <td>0.344951</td>\n",
              "      <td>-0.310451</td>\n",
              "      <td>-0.009200</td>\n",
              "      <td>0.027640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.075851</td>\n",
              "      <td>-0.028208</td>\n",
              "      <td>0.083660</td>\n",
              "      <td>0.123008</td>\n",
              "      <td>-0.100873</td>\n",
              "      <td>-0.229472</td>\n",
              "      <td>-0.039923</td>\n",
              "      <td>0.030180</td>\n",
              "      <td>-0.010829</td>\n",
              "      <td>-0.030644</td>\n",
              "      <td>...</td>\n",
              "      <td>0.143720</td>\n",
              "      <td>0.034368</td>\n",
              "      <td>-0.238438</td>\n",
              "      <td>0.020200</td>\n",
              "      <td>-0.074293</td>\n",
              "      <td>-0.142198</td>\n",
              "      <td>0.386423</td>\n",
              "      <td>-0.360985</td>\n",
              "      <td>-0.013981</td>\n",
              "      <td>-0.013176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.068081</td>\n",
              "      <td>0.044345</td>\n",
              "      <td>0.106599</td>\n",
              "      <td>0.117576</td>\n",
              "      <td>-0.236935</td>\n",
              "      <td>-0.240149</td>\n",
              "      <td>-0.052856</td>\n",
              "      <td>0.050336</td>\n",
              "      <td>-0.007994</td>\n",
              "      <td>-0.036271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.153078</td>\n",
              "      <td>-0.010070</td>\n",
              "      <td>-0.166419</td>\n",
              "      <td>0.039150</td>\n",
              "      <td>-0.072276</td>\n",
              "      <td>-0.174403</td>\n",
              "      <td>0.278976</td>\n",
              "      <td>-0.307429</td>\n",
              "      <td>-0.040699</td>\n",
              "      <td>-0.012276</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 768 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec516557-7867-44bd-97c2-32f3efeec93f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ec516557-7867-44bd-97c2-32f3efeec93f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ec516557-7867-44bd-97c2-32f3efeec93f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(pooled_embeddings.cpu().numpy()).shape"
      ],
      "metadata": {
        "id": "em-NU7qTamjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5de7ed82-124d-40e5-fd33-5e3c6c573964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(pooled_embeddings.cpu().numpy()).to_csv(\"roberta_embeddings.csv\")"
      ],
      "metadata": {
        "id": "oGdYLZeXaDR0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}